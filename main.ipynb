{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6407a9d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Celem naszego projektu było wykonanie modelu ML, który służy do rozpoznawania czy hostem danej sekwencji koronawirusa jest człowiek czy nie. Wykorzystaliśmy do tego model ProtBert do wygenerowania embeddingów sekwencji białkowych koronawirusów, a konkretnie spike protein, ponieważ wpływają one najsilniej na powinowactwo danego wirusa. Sekwencje te pobrano z bazy danych NCBI z wyłączeniem danych pochodzących z pandemii Covid-19, ponieważ wiele z nich było nieprawidłowych lub mocno nieuzupełnionych. Następnie tak wygenerowane embeddingi użyto do wytrenowania modelów RandomForest i XGBoost. Wykonano również uczenie kontrastowe modelu ProtBert w celu sprawdzenia czy dla tak specyficznego problemu z małą liczbą próbek poprawi to wyniki. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f8c41",
   "metadata": {},
   "source": [
    "Na samym początku dane pobrane w formacie FASTA sparsowano i zapisano w postaci dwóch list: jednej z samą sekwencją oraz drugą z metadanymi: nazwą sekwencji, nazwą wirusa od jakiego pochodzi dana sekwencja oraz informację czy hostem dla tego wirusa jest człowiek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_raw_path = \"data/raw/human_98.fasta\"\n",
    "nonhuman_raw_path = \"data/raw/nonhuman_98.fasta\"\n",
    "\n",
    "normal_protbert_path = \"data/processed/protbert.pkl\"\n",
    "finetuned_probert_path = \"data/processed/finetuned_protbert.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "def parse_fasta_with_groups(file_path: str, label: int) -> Tuple[List[str], List[dict]]:\n",
    "    \"\"\"Load FASTA and extracts the virus name from the header.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return [], []\n",
    "\n",
    "    sequences = []\n",
    "    metadata = []\n",
    "\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        sequence = \" \".join(list(sequence))\n",
    "\n",
    "        header = record.description\n",
    "        parts = header.split(\"|\")\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            virus_name = parts[-1].strip().strip(\".\")\n",
    "        else:\n",
    "            virus_name = \"Unknown\"\n",
    "\n",
    "        sequences.append(sequence)\n",
    "        metadata.append(\n",
    "            {\n",
    "                \"header\": header,\n",
    "                \"virus_group\": virus_name,\n",
    "                \"label\": label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(f\"Loaded {len(sequences)} sequences from {file_path}.'\")\n",
    "    return sequences, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c8de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_emb_human, metadata_human = parse_fasta_with_groups(human_raw_path, 1)\n",
    "pre_emb_nonhuman, metadata_nonhuman = parse_fasta_with_groups(nonhuman_raw_path, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e12bc",
   "metadata": {},
   "source": [
    "Następnie na podstawie wcześniej uzyskanych sekwencji wygenerowano embeddingi przy pomocy ProtBerta bez żadnego fine-tuningu, po wcześniejszym sprawdzeniu czy te embeddingi nie znajdują się już na dysku. Wygenerowane embeddingi zapisywane są jako tablica numpy, a następnie dodawana są do nich wcześniej wyciągnięte metadane: informacja odnośnie hosta, jako target oraz informacja odnośnie organizmu od jakiego pochodzi dana sekwencja w celu prawidłowego podziału danych na treningowe i testowe bez wycieku informacji. Na samym końcu dane są łączone w jednego DataFrame i zapisane w formacie pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad55733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from typing import List, Optional\n",
    "from peft import PeftModel\n",
    "import numpy as np\n",
    "\n",
    "def get_protbert_embeddings(\n",
    "    sequences_list: List[str],\n",
    "    max_seq_len: int,\n",
    "    batch_size: int,\n",
    "    adapter_path: Optional[str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Converts preprocessed samples into embeddings.\n",
    "    The batch_size parameter speeds up embedding generation, but increases the load on the computer.\n",
    "    \"\"\"\n",
    "    model_name = \"Rostlab/prot_bert\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "    base_model = BertModel.from_pretrained(model_name)\n",
    "    if adapter_path:\n",
    "        print(\"Using pre-trained adapter model\")\n",
    "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    else:\n",
    "        model = base_model\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(sequences_list), batch_size):\n",
    "        batch = sequences_list[i : i + batch_size]\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch processing: {i}/{len(sequences_list)}\")\n",
    "\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        token_embeddings = outputs.last_hidden_state\n",
    "        attention_mask = (\n",
    "            inputs[\"attention_mask\"]\n",
    "            .unsqueeze(-1)\n",
    "            .expand(token_embeddings.size())\n",
    "            .float()\n",
    "        )\n",
    "        sum_embeddings = torch.sum(token_embeddings * attention_mask, 1)\n",
    "        sum_mask = torch.clamp(attention_mask.sum(1), min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "        all_embeddings.append(mean_embeddings.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12712948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_metadata(embedding: np.ndarray, metadata: List[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Open embeddings array, add target value (e.g. 1 or 0) and specific virus name\"\"\"\n",
    "\n",
    "    virus_names = [item[\"virus_group\"] for item in metadata]\n",
    "    labels = [item[\"label\"] for item in metadata]\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        data={\n",
    "            \"embedding\": list(embedding),\n",
    "            \"label\": labels,\n",
    "            \"virus_group\": virus_names,\n",
    "        }\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c211e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_data(*embeddings: np.ndarray, out_path: str) -> None:\n",
    "    \"\"\"Concat DataFrames into one with reseted index and save them as *.pkl file\"\"\"\n",
    "    concat_data = pd.concat(embeddings, ignore_index=True)\n",
    "    folder_dir = os.path.dirname(out_path)\n",
    "    if folder_dir and not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir, exist_ok=True)\n",
    "    concat_data.to_pickle(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4427622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding were generated previously, skipping generating them.\n"
     ]
    }
   ],
   "source": [
    "NORMAL_MAX_SEQ_LEN = 1024\n",
    "NORMAL_BATCH_SIZE = 4\n",
    "\n",
    "if os.path.exists(normal_protbert_path):\n",
    "    print(\"Embedding were generated previously, skipping generating them.\")\n",
    "else:\n",
    "    protbert_emb_human = get_protbert_embeddings(pre_emb_human, NORMAL_MAX_SEQ_LEN, NORMAL_BATCH_SIZE)\n",
    "    protbert_emb_nonhuman = get_protbert_embeddings(\n",
    "        pre_emb_nonhuman, NORMAL_MAX_SEQ_LEN, NORMAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    human_labeled = add_metadata(protbert_emb_human, metadata_human)\n",
    "    nonhuman_labeled = add_metadata(protbert_emb_nonhuman, metadata_nonhuman)\n",
    "\n",
    "    concat_data(human_labeled, nonhuman_labeled, out_path=normal_protbert_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71517c21",
   "metadata": {},
   "source": [
    "Tak przygotowane embeddingi podzielono na zbiory treningowy i testowy. Zbiory te podzielono w taki sposób, aby w zbiorze treningowym nie znalazły się sekwencje pochodzące od wirusów znajdujących się w zbiorze testowym i na odwrót. Wykonano również 5-krotną walidację krzyżową w celu lepszej weryfikacji poprawności działania modelów. Następnie wykonano uczenie i weryfikację modelów Random Forest i XGBoost na embeddingach pochodzących z normalnego ProtBerta. Wyniki wyświetlane są w formie macierzy niepewności oraz raportu klasyfikacyjnego. Zapisywane są również do pliku tekstowego. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a90d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from typing import Generator, Tuple\n",
    "\n",
    "def split_train_test_virus_group(path: str, n_splits: int = 5) -> Generator[Tuple[np.ndarray, pd.Series, np.ndarray, pd.Series], None, None]:\n",
    "    data = pd.read_pickle(path)\n",
    "    groups = data[\"virus_group\"]\n",
    "    splitter = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    for train_idx, val_idx in splitter.split(data, groups=groups):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        test_df = data.iloc[val_idx]\n",
    "\n",
    "        X_train = np.stack(train_df[\"embedding\"].values)\n",
    "        y_train = train_df[\"label\"]\n",
    "\n",
    "        X_test = np.stack(test_df[\"embedding\"].values)\n",
    "        y_test = test_df[\"label\"]\n",
    "\n",
    "        yield X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def generate_report(y_true, y_predicted, result_path : str) -> Tuple[np.ndarray,  str | dict]:\n",
    "    cf = confusion_matrix(y_true, y_predicted)\n",
    "    report = classification_report(y_true, y_predicted)\n",
    "\n",
    "    with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== RESULTS ===\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(str(cf))\n",
    "        f.write(\"\\n-------------------------------\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "    return cf, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run_rf_cv_evaluation(data_path: str) -> Tuple[list, list]:\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    cv_generator = split_train_test_virus_group(data_path, n_splits=5)\n",
    "\n",
    "    for (X_train, y_train, X_val, y_val) in cv_generator:\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        all_y_true.append(y_val)\n",
    "        all_y_pred.append(y_pred)\n",
    "    \n",
    "    return all_y_true, all_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e438a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def run_xgb_cv_evaluation(data_path: str) -> Tuple[list, list]:\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    cv_generator = split_train_test_virus_group(data_path, n_splits=5)\n",
    "\n",
    "    for (X_train, y_train, X_val, y_val) in cv_generator:\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            n_estimators=100,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        all_y_true.append(y_val)\n",
    "        all_y_pred.append(y_pred)\n",
    "    \n",
    "    return all_y_true, all_y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f8e0b",
   "metadata": {},
   "source": [
    "Normal ProtBert -> Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a68314",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_rf_true, normal_rf_predicted = run_rf_cv_evaluation(normal_protbert_path)\n",
    "\n",
    "normal_rf_path = \"normal_rf_results.txt\"\n",
    "normal_rf_cm, normal_rf_report = generate_report(normal_rf_true, normal_rf_predicted, normal_rf_path)\n",
    "\n",
    "print(\"=== NORMAL PROTBERT -> RANDOM FOREST RESULTS ===\")\n",
    "print(normal_rf_cm)\n",
    "print(normal_rf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0e691",
   "metadata": {},
   "source": [
    "Normal ProtBert -> XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_xgb_true, normal_xgb_predicted = run_xgb_cv_evaluation(normal_protbert_path)\n",
    "\n",
    "normal_xgb_path = \"normal_xgb_results.txt\"\n",
    "normal_xgb_cm, normal_xgb_report = generate_report(normal_xgb_true, normal_xgb_predicted, normal_xgb_path)\n",
    "\n",
    "print(\"=== NORMAL PROTBERT -> XGBOOST RESULTS ===\")\n",
    "print(normal_xgb_cm)\n",
    "print(normal_xgb_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
