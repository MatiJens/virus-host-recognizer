{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6407a9d",
   "metadata": {},
   "source": [
    "<div style=\"text-align: justify\"> Celem naszego projektu było wykonanie modelu ML, który służy do rozpoznawania czy hostem danej sekwencji koronawirusa jest człowiek czy nie. Wykorzystaliśmy do tego model ProtBert do wygenerowania embeddingów sekwencji białkowych koronawirusów, a konkretnie spike protein, ponieważ wpływają one najsilniej na powinowactwo danego wirusa. Sekwencje te pobrano z bazy danych NCBI z wyłączeniem danych pochodzących z pandemii Covid-19, ponieważ wiele z nich było nieprawidłowych lub mocno nieuzupełnionych. Następnie tak wygenerowane embeddingi użyto do wytrenowania modelów RandomForest i XGBoost. Wykonano również uczenie kontrastowe modelu ProtBert w celu sprawdzenia czy dla tak specyficznego problemu z małą liczbą próbek poprawi to wyniki. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748f8c41",
   "metadata": {},
   "source": [
    "Na samym początku dane pobrane w formacie FASTA sparsowano i zapisano w postaci dwóch list: jednej z samą sekwencją oraz drugą z metadanymi: nazwą sekwencji, nazwą wirusa od jakiego pochodzi dana sekwencja oraz informację czy hostem dla tego wirusa jest człowiek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e8a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_raw_path = \"data/raw/human_98.fasta\"\n",
    "nonhuman_raw_path = \"data/raw/nonhuman_98.fasta\"\n",
    "\n",
    "embeddings_path = \"data/processed/protbert.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d15c6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "def parse_fasta_with_groups(file_path: str, label: int) -> Tuple[List[str], List[dict]]:\n",
    "    \"\"\"Load FASTA file, extract sequences and metadata (virus group, host) from the header and save them as two separated lists\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "        return [], []\n",
    "\n",
    "    sequences = []\n",
    "    metadata = []\n",
    "\n",
    "    for record in SeqIO.parse(file_path, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        sequence = \" \".join(list(sequence))\n",
    "\n",
    "        header = record.description\n",
    "        parts = header.split(\"|\")\n",
    "\n",
    "        if len(parts) >= 2:\n",
    "            virus_name = parts[-1].strip().strip(\".\")\n",
    "        else:\n",
    "            virus_name = \"Unknown\"\n",
    "\n",
    "        sequences.append(sequence)\n",
    "        metadata.append(\n",
    "            {\n",
    "                \"header\": header,\n",
    "                \"virus_group\": virus_name,\n",
    "                \"label\": label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(f\"Loaded {len(sequences)} sequences from {file_path}.'\")\n",
    "    return sequences, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c8de5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 134 sequences from data/raw/human_98.fasta.'\n",
      "Loaded 4507 sequences from data/raw/nonhuman_98.fasta.'\n"
     ]
    }
   ],
   "source": [
    "pre_emb_human, metadata_human = parse_fasta_with_groups(human_raw_path, 1)\n",
    "pre_emb_nonhuman, metadata_nonhuman = parse_fasta_with_groups(nonhuman_raw_path, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4e12bc",
   "metadata": {},
   "source": [
    "Następnie na podstawie wcześniej uzyskanych sekwencji wygenerowano embeddingi przy pomocy ProtBerta bez żadnego fine-tuningu, po wcześniejszym sprawdzeniu czy te embeddingi nie znajdują się już na dysku. Wygenerowane embeddingi zapisywane są jako tablica numpy, a następnie dodawana są do nich wcześniej wyciągnięte metadane: informacja odnośnie hosta, jako target oraz informacja odnośnie organizmu od jakiego pochodzi dana sekwencja w celu prawidłowego podziału danych na treningowe i testowe bez wycieku informacji. Na samym końcu dane są łączone w jednego DataFrame i zapisane w formacie pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad55733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\OneDrive\\Pulpit\\studia\\ib\\2 sem\\protBERT wirusy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from transformers import BertModel, BertTokenizer\n",
    " \n",
    "def get_protbert_embeddings(\n",
    "    sequences: List[str],\n",
    "    max_seq_len: int,\n",
    "    batch_size: int,\n",
    ") -> np.ndarray:\n",
    "    model_name = \"Rostlab/prot_bert\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "    base_model = BertModel.from_pretrained(model_name)\n",
    " \n",
    "    model = base_model\n",
    " \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    " \n",
    "    embeddings_list = []\n",
    " \n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        batch = sequences[i : i + batch_size]\n",
    "        \n",
    "        encoded_input = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_len,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    " \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "        embeddings_list.append(cls_embeddings.cpu().numpy())\n",
    " \n",
    "    return np.vstack(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12712948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def add_metadata(embedding: np.ndarray, metadata: List[dict]) -> pd.DataFrame:\n",
    "    \"\"\"Open embeddings array and add to them target value (1 for human host or 0 for nonhuman host) and virus group.\"\"\"\n",
    "\n",
    "    virus_names = [item[\"virus_group\"] for item in metadata]\n",
    "    labels = [item[\"label\"] for item in metadata]\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        data={\n",
    "            \"embedding\": list(embedding),\n",
    "            \"label\": labels,\n",
    "            \"virus_group\": virus_names,\n",
    "        }\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c211e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_data(*embeddings: np.ndarray, out_path: str) -> None:\n",
    "    \"\"\"Concat DataFrames (human and nonhuman embeddings) into one with reseted index and save them as *.pkl file\"\"\"\n",
    "    concat_data = pd.concat(embeddings, ignore_index=True)\n",
    "    folder_dir = os.path.dirname(out_path)\n",
    "    if folder_dir and not os.path.exists(folder_dir):\n",
    "        os.makedirs(folder_dir, exist_ok=True)\n",
    "    concat_data.to_pickle(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4427622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding were generated previously, skipping generating them.\n"
     ]
    }
   ],
   "source": [
    "NORMAL_MAX_SEQ_LEN = 1024\n",
    "NORMAL_BATCH_SIZE = 4\n",
    "\n",
    "if os.path.exists(embeddings_path):\n",
    "    print(\"Embedding were generated previously, skipping generating them.\")\n",
    "else:\n",
    "    protbert_emb_human = get_protbert_embeddings(pre_emb_human, NORMAL_MAX_SEQ_LEN, NORMAL_BATCH_SIZE)\n",
    "    protbert_emb_nonhuman = get_protbert_embeddings(\n",
    "        pre_emb_nonhuman, NORMAL_MAX_SEQ_LEN, NORMAL_BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    human_labeled = add_metadata(protbert_emb_human, metadata_human)\n",
    "    nonhuman_labeled = add_metadata(protbert_emb_nonhuman, metadata_nonhuman)\n",
    "\n",
    "    concat_data(human_labeled, nonhuman_labeled, out_path=embeddings_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71517c21",
   "metadata": {},
   "source": [
    "Tak przygotowane embeddingi podzielono na zbiory treningowy i testowy. Zbiory te podzielono w taki sposób, aby w zbiorze treningowym nie znalazły się sekwencje pochodzące od wirusów znajdujących się w zbiorze testowym i na odwrót. Wykonano również 5-krotną walidację krzyżową w celu lepszej weryfikacji poprawności działania modelów. Następnie wykonano uczenie i weryfikację modelów Random Forest i XGBoost na embeddingach pochodzących z normalnego ProtBerta. Wyniki wyświetlane są w formie macierzy niepewności oraz raportu klasyfikacyjnego. Zapisywane są również do pliku tekstowego. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a90d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from typing import Generator, Tuple\n",
    "\n",
    "def split_train_test_virus_group(path: str, n_splits: int = 5) -> Generator[Tuple[np.ndarray, pd.Series, np.ndarray, pd.Series], None, None]:\n",
    "    \"\"\"Divides the data into training and test sets, ensuring that sequences originating from a single virus are only included in one group. The generator is designed to perform cross-validation.\"\"\"\n",
    "    data = pd.read_pickle(path)\n",
    "    groups = data[\"virus_group\"]\n",
    "    splitter = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "    for train_idx, val_idx in splitter.split(data, groups=groups):\n",
    "        train_df = data.iloc[train_idx]\n",
    "        test_df = data.iloc[val_idx]\n",
    "\n",
    "        X_train = np.stack(train_df[\"embedding\"].values)\n",
    "        y_train = train_df[\"label\"]\n",
    "\n",
    "        X_test = np.stack(test_df[\"embedding\"].values)\n",
    "        y_test = test_df[\"label\"]\n",
    "\n",
    "        yield X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f88f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def generate_report(y_true, y_predicted, result_path : str) -> Tuple[np.ndarray,  str | dict]:\n",
    "    cf = confusion_matrix(y_true, y_predicted)\n",
    "    report = classification_report(y_true, y_predicted)\n",
    "\n",
    "    with open(result_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== RESULTS ===\\n\")\n",
    "        f.write(\"Confusion Matrix:\\n\")\n",
    "        f.write(str(cf))\n",
    "        f.write(\"\\n-------------------------------\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "    return cf, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def run_rf_cv_evaluation(data_path: str) -> Tuple[list, list]:\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    cv_generator = split_train_test_virus_group(data_path, n_splits=5)\n",
    "\n",
    "    for (X_train, y_train, X_test, y_test) in cv_generator:\n",
    "\n",
    "        model = RandomForestClassifier(\n",
    "            n_estimators=200,\n",
    "            min_samples_leaf=2,\n",
    "            max_features=10,\n",
    "            max_depth=10,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        all_y_true.append(y_test)\n",
    "        all_y_pred.append(y_pred)\n",
    "    \n",
    "    return np.concatenate(all_y_true), np.concatenate(all_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e438a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def run_xgb_cv_evaluation(data_path: str) -> Tuple[list, list]:\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    cv_generator = split_train_test_virus_group(data_path, n_splits=5)\n",
    "\n",
    "    for (X_train, y_train, X_val, y_val) in cv_generator:\n",
    "\n",
    "        model = XGBClassifier(\n",
    "            subsample=1.0,\n",
    "            min_child_weight=1,\n",
    "            max_depth=7,\n",
    "            learning_rate=0.1,\n",
    "            colsample_bytree=0.6,\n",
    "            n_estimators=100,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        all_y_true.append(y_val)\n",
    "        all_y_pred.append(y_pred)\n",
    "    \n",
    "    return np.concatenate(all_y_true), np.concatenate(all_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a907ab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def run_svm_cv_evaluation(data_path: str) -> Tuple[list, list]:\n",
    "    all_y_true = []\n",
    "    all_y_pred = []\n",
    "\n",
    "    cv_generator = split_train_test_virus_group(data_path, n_splits=5)\n",
    "\n",
    "    for (X_train, y_train, X_val, y_val) in cv_generator:\n",
    "\n",
    "        model = SVC(kernel=\"rbf\", gamma=1, C=1000, class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        all_y_true.append(y_val)\n",
    "        all_y_pred.append(y_pred)\n",
    "    \n",
    "    return np.concatenate(all_y_true), np.concatenate(all_y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f8e0b",
   "metadata": {},
   "source": [
    "Wyniki Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a68314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NORMAL PROTBERT -> RANDOM FOREST RESULTS ===\n",
      "[[4504    3]\n",
      " [ 122   12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      4507\n",
      "           1       0.80      0.09      0.16       134\n",
      "\n",
      "    accuracy                           0.97      4641\n",
      "   macro avg       0.89      0.54      0.57      4641\n",
      "weighted avg       0.97      0.97      0.96      4641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normal_rf_true, normal_rf_predicted = run_rf_cv_evaluation(embeddings_path)\n",
    "\n",
    "normal_rf_path = \"normal_rf_results.txt\"\n",
    "normal_rf_cm, normal_rf_report = generate_report(normal_rf_true, normal_rf_predicted, normal_rf_path)\n",
    "\n",
    "print(\"=== RANDOM FOREST RESULTS ===\")\n",
    "print(normal_rf_cm)\n",
    "print(normal_rf_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0e691",
   "metadata": {},
   "source": [
    "Wyniki XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74d2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateu\\OneDrive\\Pulpit\\studia\\ib\\2 sem\\protBERT wirusy\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:12:02] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"class_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\mateu\\OneDrive\\Pulpit\\studia\\ib\\2 sem\\protBERT wirusy\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:12:05] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"class_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\mateu\\OneDrive\\Pulpit\\studia\\ib\\2 sem\\protBERT wirusy\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:12:10] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"class_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\mateu\\OneDrive\\Pulpit\\studia\\ib\\2 sem\\protBERT wirusy\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:12:14] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"class_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\mateu\\OneDrive\\Pulpit\\studia\\ib\\2 sem\\protBERT wirusy\\.venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [15:12:17] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"class_weight\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NORMAL PROTBERT -> XGBOOST RESULTS ===\n",
      "[[4497   10]\n",
      " [ 110   24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      4507\n",
      "           1       0.71      0.18      0.29       134\n",
      "\n",
      "    accuracy                           0.97      4641\n",
      "   macro avg       0.84      0.59      0.64      4641\n",
      "weighted avg       0.97      0.97      0.97      4641\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normal_xgb_true, normal_xgb_predicted = run_xgb_cv_evaluation(embeddings_path)\n",
    "\n",
    "normal_xgb_path = \"normal_xgb_results.txt\"\n",
    "normal_xgb_cm, normal_xgb_report = generate_report(normal_xgb_true, normal_xgb_predicted, normal_xgb_path)\n",
    "\n",
    "print(\"=== XGBOOST RESULTS ===\")\n",
    "print(normal_xgb_cm)\n",
    "print(normal_xgb_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec17a30",
   "metadata": {},
   "source": [
    "Wyniki SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_svm_true, normal_svm_predicted = run_svm_cv_evaluation(embeddings_path)\n",
    "\n",
    "normal_svm_path = \"normal_svm_results.txt\"\n",
    "normal_svm_cm, normal_svm_report = generate_report(normal_svm_true, normal_svm_predicted, normal_svm_path)\n",
    "\n",
    "print(\"=== SVM RESULTS ===\")\n",
    "print(normal_svm_cm)\n",
    "print(normal_svm_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
